{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65700bd-79dd-4a6a-a961-99a64c6ddf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list tuple decorators generators\n",
    "# random forest in general \n",
    "# anamoly and outlier diff\n",
    "\n",
    "# coding questions :\n",
    "    # palindrome\n",
    "    # string\n",
    "    # and list \n",
    "    # pattern matching\n",
    "    # regex \n",
    "\n",
    "\n",
    "\n",
    "# ML : \n",
    "    # curse of dimentionlaity\n",
    " \n",
    "# there is one column in thrt are 1000 of  categories(classes) if u do one hot encoding then it will increase columns \n",
    "#  we want our data less it is increasing so which is against rule how will tackle this problem\n",
    "\n",
    "# we can use label encoder but we use label encoder when we need ordinal relationship\n",
    "# not a perfect solution\n",
    "# so\n",
    "\n",
    "# we need to categories that classes into smaller if there is species then name it and sort it like mammels 10\n",
    "# ambhibius 20 like this\n",
    "# reptiles 30 etc\n",
    "\n",
    "# in technical  round all bookish knowledge \n",
    "# py 3.7 and 3.10 diff\n",
    "# 2.7 an 3.0 diff\n",
    "# py is synchronous or asynchronus language\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# in  managerial round\n",
    "\n",
    "# what is your work \n",
    "# roles and responsibilities\n",
    "# not ur team u only\n",
    "\n",
    "# project work what u did\n",
    "\n",
    "# how much data u worked on?\n",
    "# how many columns and featueres of them\n",
    "# his project was supply chain analytics\n",
    "\n",
    "# so interviwer asked him in your project what were the features of of columns\n",
    "\n",
    "# so he answered we got data from multiple sources\n",
    "# there were many types of data like inventory data,ordered,logistics data,shipment data, etc \n",
    "# in ordered data there were  order id, amount id, product id, transaction kis mode se hua tha\n",
    "# inventory id me product hai uske kitne unit apne pass hai\n",
    "# kab last order tha\n",
    "# current quantity kitni hai \n",
    "# ordered data hai matlab tumhara org kitne oreder le raha hai\n",
    "# 7k -8k \n",
    "# logistic data 3k oreders ship kar rahe ho to bolne ka roz ke 3k column pe hum kaam kar rahe the\n",
    "\n",
    "# string is given check karo wo palindrome hai ya nahi\n",
    "# aur mujhe uska apalindrome score chaiye\n",
    "\n",
    "# data tranformation ke kaunse kaunse technique hai\n",
    "\n",
    "\n",
    "# if u don't know anything\n",
    "# I never came across this scenario so sorry i don't know\n",
    "# right now i m not able to racall\n",
    "# but if u me chance i'll  explore it my self & work on it\n",
    "\n",
    "# task management tool\n",
    "aws glue\n",
    "airflow\n",
    "informitica \n",
    "\n",
    "\n",
    "\n",
    "# kantilal sir said 99% scenario don't ask question to interviwer never\n",
    "\n",
    "#if he asks u want to ask anything then only ask without then dont ask Q to interviwer \n",
    "\n",
    "# i ve never used informitica but i've used same tool like this is etl is aws glue\n",
    "# question u want to ask anything ? \n",
    "# is there any hope for me for this role\n",
    "# may I know the job disription and must skill sets for this role\n",
    "\n",
    "# team work\n",
    "# always say our team has done this and that\n",
    "# there is team leader he assigned us task\n",
    "# and we are working on that task \n",
    "# this is all colaborative work of us\n",
    "\n",
    "\n",
    "# HR ko kuch bhi pata nahi hota usko sirf ha bolo\n",
    "\n",
    "# notice period 15 days or 3 months \n",
    "\n",
    "# my manager will relieve I already had adiscussion with my manager ,\n",
    "# may be there will be delay of 1 day or 2 but I will  join within 15-20 days\n",
    "\n",
    "\n",
    "# HR :what is ur expected salary\n",
    "\n",
    "# 8.3 package\n",
    "# 15 tak mangana hai\n",
    "# at leat 50-60% hike\n",
    "# HR: plz tell me the digit\n",
    "\n",
    "# why we give u this amount?\n",
    "# I've recently worked on aws i've added snowflake,azure in my current skillset\n",
    "\n",
    "# Kantilal sir :\n",
    "\n",
    "# variable ko mana kar do sidha\n",
    "\n",
    "# HR log jhase me late hai \n",
    "# 16 lpa ok but including variable denge\n",
    "# say no to this\n",
    "# fixed chahiye \n",
    "\n",
    "# variable dala to wo 3 lakh ka variable dal denge CLI(company linked incentive ) 13 lakh ka package ho jayega phir wo\n",
    "# company ka performance achha raha to hi milta hai nahi to nahi milta hai\n",
    "\n",
    "\n",
    "# if u get 12lpa still then u get 75k-80k in hand only \n",
    "\n",
    "\n",
    "# if interview goes fine u can ask HR to give joining bonus for immidiate joining\n",
    "# REMEMBER\n",
    "# agar aap kuch perks le rahe ho aur within 6 months ya 1 yr ke andar chhod dete ho to vo revert bhi hota hai\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e5b12-6573-4686-9c95-edd4e5e1f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data tranformation ke kaunse kaunse technique hai\n",
    "\n",
    "\n",
    "\n",
    "# Data transformation is a critical step in the data science process, where raw data is manipulated or converted to make it more suitable for analysis. There are various techniques used in data transformation, some of which include:\n",
    "\n",
    "# 1. **Normalization**: This technique scales the numeric features in a dataset to a standard range, typically between 0 and 1 or -1 and 1. It helps in handling features with different scales and units, ensuring that no single feature dominates the others in the analysis.\n",
    "\n",
    "# 2. **Standardization**: Also known as Z-score normalization, standardization transforms data to have a mean of 0 and a standard deviation of 1. It's particularly useful when the features have different distributions or magnitudes.\n",
    "\n",
    "# 3. **Log Transformation**: When data is skewed or does not follow a normal distribution, log transformation can be applied to make it more symmetric and to stabilize variance. This is especially common in financial and economic data analysis.\n",
    "\n",
    "# 4. **Box-Cox Transformation**: Similar to log transformation, the Box-Cox transformation is used to stabilize variance and make the data more normally distributed. It is more flexible as it can handle both positive and negative values and select the best transformation parameter lambda automatically.\n",
    "\n",
    "# 5. **Feature Scaling**: In addition to normalization and standardization, other scaling techniques such as Min-Max scaling, where data is scaled to a specific range, can be used based on the requirements of the algorithm being used.\n",
    "\n",
    "# 6. **Encoding Categorical Variables**: Categorical variables need to be converted into numerical form for analysis. Techniques like one-hot encoding, where each category is represented by a binary vector, or label encoding, where each category is assigned a unique integer, are commonly used.\n",
    "\n",
    "# 7. **Imputation**: Dealing with missing data is a crucial aspect of data transformation. Imputation techniques such as mean, median, mode imputation, or more advanced methods like K-nearest neighbors (KNN) or multiple imputation are used to fill in missing values.\n",
    "\n",
    "# 8. **Binning/Discretization**: Binning involves grouping continuous numerical data into bins or intervals. This can help in reducing the effects of outliers and noise in the data, and it can also simplify the model complexity.\n",
    "\n",
    "# 9. **Feature Engineering**: This involves creating new features from existing ones to enhance the predictive power of the model. Techniques include polynomial features, interaction terms, and transformations like square, cube, etc.\n",
    "\n",
    "# 10. **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) are used to reduce the dimensionality of the data while retaining most of the information. This can help in simplifying models and reducing computational complexity.\n",
    "\n",
    "# These techniques are often applied iteratively and in combination to preprocess the data effectively before feeding it into machine learning algorithms. The choice of technique depends on the nature of the data, the requirements of the analysis, and the specific characteristics of the machine learning model being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97bfd8-9f52-416e-b08c-0c3a2928c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ec27c-1e4b-451a-bd12-108ba04ba936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sure! The chi-square test is a statistical test used to determine whether there is a significant association between two categorical variables. It helps answer questions like \"Are these two categorical variables related?\" or \"Is there a difference in the distribution of categories between two groups?\"\n",
    "\n",
    "# Here's a simplified explanation along with the formula:\n",
    "\n",
    "# **Chi-Square Test Formula**:\n",
    "\n",
    "# \\[\n",
    "# \\chi^2 = \\sum \\frac{(O - E)^2}{E}\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\(\\chi^2\\) is the chi-square statistic.\n",
    "# - \\(O\\) is the observed frequency (the actual count in each category).\n",
    "# - \\(E\\) is the expected frequency (the count we would expect if there were no relationship between the variables).\n",
    "\n",
    "# **Explanation**:\n",
    "\n",
    "# 1. **Observed Frequency (\\(O\\))**: This is the actual count of observations in each category of the contingency table.\n",
    "\n",
    "# 2. **Expected Frequency (\\(E\\))**: This is the count we would expect in each category if there were no association between the variables. It's calculated based on the assumption of independence between the variables. To find the expected frequency for each cell, you typically use the formula:\n",
    "\n",
    "# \\[\n",
    "# E = \\frac{\\text{row total} \\times \\text{column total}}{\\text{grand total}}\n",
    "# \\]\n",
    "\n",
    "# 3. **Chi-Square Statistic (\\(\\chi^2\\))**: This is the measure of how much the observed counts differ from the expected counts. It's calculated by summing up the squared differences between the observed and expected frequencies, divided by the expected frequency for each category.\n",
    "\n",
    "# 4. **Degrees of Freedom (df)**: Degrees of freedom are used to determine the critical value from the chi-square distribution table. For a chi-square test of independence, the degrees of freedom is calculated as \\((r - 1) \\times (c - 1)\\), where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the contingency table.\n",
    "\n",
    "# 5. **Interpreting the Result**: Once you've calculated the chi-square statistic, you compare it to the critical value from the chi-square distribution table. If the calculated chi-square value is greater than the critical value, it indicates that there is a significant association between the variables. In other words, the variables are not independent. If the calculated chi-square value is less than the critical value, there is no significant association, and the variables are independent.\n",
    "\n",
    "# In simpler terms, the chi-square test helps us determine whether the differences between observed and expected frequencies are too large to be explained by chance alone, suggesting that there might be a real relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5b76c-6d1e-4090-b679-816fc2d7a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA\n",
    "\n",
    "# Sure! ANOVA, or Analysis of Variance, is a statistical test used to compare the means of three or more groups to determine whether they are significantly different from each other. It helps answer questions like \"Are there any differences between the means of these groups?\" or \"Does this factor have a significant effect on the outcome?\"\n",
    "\n",
    "# Here's a simplified explanation along with the formula:\n",
    "\n",
    "# **ANOVA Formula**:\n",
    "\n",
    "# \\[\n",
    "# F = \\frac{{\\text{Between-group variance}}}{{\\text{Within-group variance}}}\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\(F\\) is the F-statistic, which follows an F-distribution.\n",
    "# - Between-group variance measures the variation between the means of different groups.\n",
    "# - Within-group variance measures the variation within each group.\n",
    "\n",
    "# **Explanation**:\n",
    "\n",
    "# 1. **Between-group Variance**: This measures the variability in the means of different groups. It compares how much each group's mean differs from the overall mean. If the means of the groups are very different from each other, the between-group variance will be large.\n",
    "\n",
    "# 2. **Within-group Variance**: This measures the variability within each group. It reflects how much individual data points deviate from their group's mean. If the data points within each group are tightly clustered around their respective means, the within-group variance will be small.\n",
    "\n",
    "# 3. **F-statistic**: The F-statistic is calculated by dividing the between-group variance by the within-group variance. It tells us whether the differences between the group means are statistically significant or just due to random variation. If the F-statistic is large, it suggests that the means of at least some groups are significantly different from each other.\n",
    "\n",
    "# 4. **Degrees of Freedom (df)**: In ANOVA, there are two degrees of freedom: one for the between-group variance and one for the within-group variance. The degrees of freedom are used to determine the critical value from the F-distribution table.\n",
    "\n",
    "# 5. **Interpreting the Result**: Once you've calculated the F-statistic, you compare it to the critical value from the F-distribution table. If the calculated F-statistic is greater than the critical value, it indicates that there are significant differences between the means of at least some groups. If the calculated F-statistic is less than the critical value, there are no significant differences between the means of the groups.\n",
    "\n",
    "# In simpler terms, ANOVA helps us determine whether there are any real differences in the means of the groups, or if the differences we observe are just due to random chance. It's like comparing the differences between groups to the differences within each group to see if they're big enough to be considered meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552e784-0bf3-44e0-9526-3a6813ecf4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # give me python libaries\n",
    "\n",
    "# Certainly! Here are some popular Python libraries used in data science and machine learning:\n",
    "\n",
    "# 1. **NumPy**: Fundamental package for numerical computing with arrays and matrices, along with a large collection of mathematical functions.\n",
    "\n",
    "# 2. **Pandas**: Provides high-performance, easy-to-use data structures and data analysis tools for manipulating structured data (e.g., tabular data, time series data).\n",
    "\n",
    "# 3. **Matplotlib**: Comprehensive library for creating static, interactive, and animated visualizations in Python.\n",
    "\n",
    "# 4. **Seaborn**: Built on top of Matplotlib, Seaborn provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "# 5. **Scikit-learn**: Simple and efficient tools for data mining and data analysis. It includes various machine learning algorithms, preprocessing techniques, and model evaluation methods.\n",
    "\n",
    "# 6. **TensorFlow**: An open-source machine learning framework developed by Google for building and training deep learning models.\n",
    "\n",
    "# 7. **PyTorch**: Another popular open-source machine learning framework, developed by Facebook's AI Research lab, primarily used for building deep learning models.\n",
    "\n",
    "# 8. **Keras**: High-level neural networks API, written in Python and capable of running on top of TensorFlow, Theano, or Microsoft Cognitive Toolkit (CNTK).\n",
    "\n",
    "# 9. **SciPy**: A collection of mathematical algorithms and convenience functions built on the NumPy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.\n",
    "\n",
    "# 10. **Statsmodels**: A Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.\n",
    "\n",
    "# 11. **NLTK (Natural Language Toolkit)**: A leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "\n",
    "# 12. **Gensim**: A robust library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.\n",
    "\n",
    "# These libraries cover a wide range of tasks in data science, including data manipulation, visualization, machine learning, deep learning, natural language processing, and statistical analysis. Depending on your specific needs, you may find one or more of these libraries useful for your projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
